{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwSfrkQf2cpzqcvlc2v4r7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninenine-9/legaldetainment/blob/main/v2_fewshot_COT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõú GitHub connection"
      ],
      "metadata": {
        "id": "vOHByVTuwHdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For the 1st time"
      ],
      "metadata": {
        "id": "zrrYARZcw9MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to your desired projects folder in Google Drive\n",
        "# (You can change 'Colab_Projects' to whatever you like)\n",
        "%cd /content/drive/MyDrive/Colab_Projects/\n",
        "\n",
        "# Clone the repository directly into this folder\n",
        "!git clone https://github.com/ninenine-9/legaldetainment.git"
      ],
      "metadata": {
        "id": "Vyv0_FCSwzkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Everyother time"
      ],
      "metadata": {
        "id": "Uexs9LHRw-5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate directly to your project folder inside Google Drive\n",
        "%cd /content/drive/MyDrive/Colab_Projects/legaldetainment/\n",
        "\n",
        "# Pull the latest changes from your GitHub repo\n",
        "!git pull"
      ],
      "metadata": {
        "id": "yFVQzEC1xAdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "6taKEM7mwyVw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LwaT7hmvLod"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q -U transformers bitsandbytes accelerate\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import json\n",
        "\n",
        "print(\"‚úîÔ∏è Imports completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading\n",
        "_(approx 4 mins)_"
      ],
      "metadata": {
        "id": "rK0_PxR6I8FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load the LLM and Tokenizer ---\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Configure quantization to load the model in 4-bit\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"  # Automatically maps the model to the available GPU\n",
        ")\n",
        "\n",
        "print(\"‚úîÔ∏è LLM and Tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "id": "BvvOg8ievUCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "_-Di-1SJKDHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data retrieval\n",
        "# from google.colab import drive\n",
        "\n",
        "# %cd /content\n",
        "\n",
        "# !git clone https://github.com/ninenine-9/legaldetainment.git\n",
        "\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# !cp -r /content/legaldetainment /content/drive/MyDrive/\n",
        "\n",
        "# print(\"‚úîÔ∏è Git Repository successfully cloned\")\n",
        "\n",
        "# # with open(\"/content/legaldetainment/INPUTS/legaldetainment_story.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "# #     study_context = f.read()\n",
        "\n",
        "# # with open(\"/content/legaldetainment/INPUTS/legaldetainment_codinginstructions.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "# #     coding_manual = f.read()\n",
        "\n",
        "# Data loading\n",
        "\n",
        "blank_df = pd.read_excel(\"/content/legaldetainment/DATA/legaldetainment_blankdata.xlsx\", sheet_name = '30participants') # üìã for cell H & G\n",
        "\n",
        "blank_df = blank_df.dropna(subset=[\"Pno\"])\n",
        "blank_df.columns = blank_df.columns.str.strip()\n",
        "\n",
        "blank_df = blank_df[[\"Pno\", \"Qual\", \"GuiltMoreLikely\",\n",
        "        \"GuiltLessLikely\",\n",
        "        \"NoInformation_Evidence\",\n",
        "        \"InnocentUntilProvenGuilty\",\n",
        "        \"Confound\"]]\n",
        "\n",
        "blank_df = blank_df.head(1)\n",
        "\n",
        "print(f\"Dataset size: {blank_df.shape}\")\n",
        "\n",
        "humandata = pd.read_excel(\"/content/legaldetainment/DATA/legaldetainment_humandata.xlsx\", sheet_name = '30participants') # üìã for cell I\n",
        "\n",
        "humandata = humandata.dropna(subset=[\"Pno\"])\n",
        "humandata = humandata.replace({0: \"No\", 1: \"Yes\", \"0\": \"No\", \"1\": \"Yes\"})\n",
        "humandata = humandata.rename(columns={\n",
        "    \"NoInformation/Evidence\": \"NoInformation_Evidence\",\n",
        "    \"Unclassified/Other\": \"Unclassified_Other\"\n",
        "})\n",
        "humandata.columns = humandata.columns.str.strip()\n",
        "\n",
        "humandata = humandata[[\"Pno\", \"Qual\", \"GuiltMoreLikely\",\n",
        "        \"GuiltLessLikely\",\n",
        "        \"NoInformation_Evidence\",\n",
        "        \"InnocentUntilProvenGuilty\",\n",
        "        \"Confound\"]]\n",
        "\n",
        "humandata = humandata.head(1)\n",
        "\n",
        "print(\"‚úîÔ∏è Data loading done\")"
      ],
      "metadata": {
        "id": "OeQF3yAaKI6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codebook"
      ],
      "metadata": {
        "id": "35Dvz4pIKMgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM-Adapted Codebook\n",
        "# This structured list of dictionaries translates your human-centric coding instructions into a\n",
        "# machine-readable format. It is the foundation for building high-performance prompts.\n",
        "\n",
        "llm_codebook = [\n",
        "    {\n",
        "        \"Code_Name\": \"GuiltMoreLikely\",\n",
        "        \"Simple_Definition\": \"This code applies when the participant concludes the defendant is more likely to be guilty because they infer the judge's decision to *deny bail* was based on secret, negative information (like a criminal history).\",\n",
        "        \"Inclusion_Criteria\": \"\"\"- The text must connect the judge's decision to **deny bail** (or keep the defendant detained) to a higher likelihood of guilt.\n",
        "- The text must state or imply that the judge's decision is based on extra information that the participant does not have (e.g., 'the judge must know something,' 'he probably has a record').\"\"\",\n",
        "        \"Exclusion_Criteria\": \"\"\"- DO NOT apply if the participant mentions the judge *granting* bail.\n",
        "- DO NOT apply if the participant simply says the defendant is guilty without linking it to the judge's bail decision.\n",
        "- DO NOT apply if the participant says there is not enough information to make a judgment (this would be NoInformation_Evidence).\n",
        "- DO NOT apply if the participant's reasoning is about the judge being biased (this would be Confound).\"\"\",\n",
        "        \"Positive_Exemplar_Text\": \"The judge wouldn't have kept him in jail if there wasn't something else going on. He probably has a criminal history, so I think he's more likely to be guilty.\",\n",
        "        \"Positive_Exemplar_Explanation\": \"This meets all criteria: it mentions detainment, infers the judge knows about a 'criminal history,' and concludes guilt is more likely.\",\n",
        "        \"Negative_Exemplar_Text\": \"He was denied bail, so the judge is probably biased against him now.\",\n",
        "        \"Negative_Exemplar_Explanation\": \"This is a hard negative. It mentions bail denial but focuses on future judge bias, which fits the `Confound` code, not the inference of secret evidence required for `GuiltMoreLikely`.\"\n",
        "    },\n",
        "    {\n",
        "        \"Code_Name\": \"GuiltLessLikely\",\n",
        "        \"Simple_Definition\": \"This code applies when the participant concludes the defendant is less likely to be guilty because they infer the judge's decision to *grant bail* was based on secret, positive information (like a solid alibi).\",\n",
        "        \"Inclusion_Criteria\": \"\"\"- The text must connect the judge's decision to **grant bail** (or release the defendant) to a lower likelihood of guilt.\n",
        "- The text must state or imply that the judge's decision is based on extra information that the participant does not have (e.g., 'the judge must have seen the alibi').\"\"\",\n",
        "        \"Exclusion_Criteria\": \"\"\"- DO NOT apply if the participant mentions the judge *denying* bail.\n",
        "- DO NOT apply if the participant simply says the defendant is innocent without linking it to the judge's bail decision.\n",
        "- DO NOT apply if the participant says there is not enough information to make a judgment (this would be NoInformation_Evidence).\"\"\",\n",
        "        \"Positive_Exemplar_Text\": \"If the judge let him go, he must have a solid alibi or something. I'd say he's less likely to be guilty.\",\n",
        "        \"Positive_Exemplar_Explanation\": \"This meets all criteria: it mentions release, infers the judge knows about a 'solid alibi,' and concludes guilt is less likely.\",\n",
        "        \"Negative_Exemplar_Text\": \"It's good he got bail, but we should remember he is innocent until proven guilty anyway.\",\n",
        "        \"Negative_Exemplar_Explanation\": \"This is a hard negative. Although it mentions bail, the core reasoning invokes the legal principle of `InnocentUntilProvenGuilty`, which is a different code.\"\n",
        "    },\n",
        "    {\n",
        "        \"Code_Name\": \"NoInformation_Evidence\",\n",
        "        \"Simple_Definition\": \"This code applies when the participant explicitly states that they cannot make a judgment because there is not enough information or evidence provided.\",\n",
        "        \"Inclusion_Criteria\": \"\"\"- The text must contain phrases that explicitly state a lack of information or evidence, such as 'not enough information,' 'no evidence,' 'can't decide based on this,' or 'need more facts.'\"\"\",\n",
        "        \"Exclusion_Criteria\": \"\"\"- DO NOT apply if the participant makes a judgment about guilt or innocence, even if they express some uncertainty (e.g., 'I guess he's guilty, but it's hard to say').\n",
        "- DO NOT apply if the participant is simply confused about the question itself; the focus must be on the lack of case evidence.\"\"\",\n",
        "        \"Positive_Exemplar_Text\": \"Based on this, I can't say. There's not enough evidence here to make a call.\",\n",
        "        \"Positive_Exemplar_Explanation\": \"This is a direct statement about the lack of evidence preventing a judgment.\",\n",
        "        \"Negative_Exemplar_Text\": \"I lean towards him being guilty, but I don't really have enough information.\",\n",
        "        \"Negative_Exemplar_Explanation\": \"This is a hard negative because even though the participant mentions a lack of information, they still make a judgment ('lean towards him being guilty'), so this code does not apply.\"\n",
        "    },\n",
        "    {\n",
        "        \"Code_Name\": \"InnocentUntilProvenGuilty\",\n",
        "        \"Simple_Definition\": \"This code applies when the participant references the legal principle that a person is considered innocent until their guilt is proven in court, regardless of other factors.\",\n",
        "        \"Inclusion_Criteria\": \"\"\"- The text must explicitly use the phrase 'innocent until proven guilty' or a very close variation (e.g., 'innocent till guilty').\n",
        "- OR, the text must clearly express the same sentiment, such as 'we shouldn't prejudge him,' or 'none of this matters until the trial proves he did it.'\"\"\",\n",
        "        \"Exclusion_Criteria\": \"\"\"- DO NOT apply if the participant simply says the defendant is 'innocent' without referencing the legal principle or the idea of waiting for proof at trial.\"\"\",\n",
        "        \"Positive_Exemplar_Text\": \"It doesn't matter what the judge decided about bail, he is innocent until proven guilty.\",\n",
        "        \"Positive_Exemplar_Explanation\": \"This is a perfect example as it uses the exact key phrase to state the legal principle.\",\n",
        "        \"Negative_Exemplar_Text\": \"I think he's innocent.\",\n",
        "        \"Negative_Exemplar_Explanation\": \"This is a hard negative because it states an opinion on innocence but does not invoke the specific legal principle of 'innocent until proven guilty'.\"\n",
        "    },\n",
        "    {\n",
        "        \"Code_Name\": \"Confound\",\n",
        "        \"Simple_Definition\": \"This code applies when the participant expresses concern about the fairness of the main trial because the same judge will be involved and may now be biased due to the bail decision.\",\n",
        "        \"Inclusion_Criteria\": \"\"\"- The text must state that the **same judge** from the bail hearing will also preside over the main trial.\n",
        "- AND/OR the text must state that the judge might now be **biased** (either for or against the defendant) as a direct result of the bail decision.\"\"\",\n",
        "        \"Exclusion_Criteria\": \"\"\"- DO NOT apply for general statements about the legal system being unfair. The comment must be specific to the judge's potential bias from this specific situation.\n",
        "- DO NOT apply if the participant thinks the judge has secret information but doesn't mention bias (this would be `GuiltMoreLikely` or `GuiltLessLikely`).\"\"\",\n",
        "        \"Positive_Exemplar_Text\": \"Since it's the same judge for the trial, he might be biased against the defendant now that he's denied him bail.\",\n",
        "        \"Positive_Exemplar_Explanation\": \"This meets both potential criteria: it mentions the 'same judge' and explicitly states the risk of bias.\",\n",
        "        \"Negative_Exemplar_Text\": \"Because each judge is new to the case they will have fresh eyes\",\n",
        "        \"Negative_Exemplar_Explanation\": \"This is a hard negative because it states an opinion on innocence but does not invoke the specific legal principle of 'innocent until proven guilty'.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "LNiiCy7LZUY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üö® Prompt generation"
      ],
      "metadata": {
        "id": "cM4JD8_XKPGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBABY MAIN PROBLEM\n",
        "\n",
        "def construct_high_performance_prompt(text_to_analyze, code_entry):\n",
        "    \"\"\"\n",
        "    Constructs a detailed, few-shot prompt with Chain-of-Thought examples\n",
        "    for a single code classification task.\n",
        "\n",
        "    Args:\n",
        "        text_to_analyze (str): The participant's text to be coded.\n",
        "        code_entry (dict): A dictionary from the llm_codebook for a single code.\n",
        "\n",
        "    Returns:\n",
        "        str: A fully formatted prompt ready for the LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the criteria lists into bulleted strings for clarity in the prompt\n",
        "    inclusion_criteria_str = \"\\n\".join([f\"- {item}\" for item in code_entry['Inclusion_Criteria']])\n",
        "    exclusion_criteria_str = \"\\n\".join([f\"- {item}\" for item in code_entry['Exclusion_Criteria']])\n",
        "\n",
        "    # --- Construct the Chain-of-Thought reasoning for the few-shot examples ---\n",
        "    # This is where we \"teach\" the model how to think by showing our work.\n",
        "\n",
        "    # Reasoning for the positive example\n",
        "    positive_reasoning = (\n",
        "        f\"Step 1: The code '{code_entry['Code_Name']}' is about '{code_entry}'. \"\n",
        "        f\"Step 2: The provided text is '{code_entry}'. \"\n",
        "        f\"Step 3: I will evaluate this against the criteria. The text meets the inclusion criteria because {code_entry['Positive_Exemplar_Explanation']}. \"\n",
        "        f\"Step 4: Therefore, the code applies.\"\n",
        "    )\n",
        "\n",
        "    # Reasoning for the negative example\n",
        "    negative_reasoning = (\n",
        "        f\"Step 1: The code '{code_entry['Code_Name']}' is about '{code_entry}'. \"\n",
        "        f\"Step 2: The provided text is '{code_entry}'. \"\n",
        "        f\"Step 3: I will evaluate this against the criteria. The text does not meet the inclusion criteria because {code_entry['Negative_Exemplar_Explanation']}. \"\n",
        "        f\"Step 4: Therefore, the code does not apply.\"\n",
        "    )\n",
        "\n",
        "    # --- Assemble the final prompt using an f-string and XML tags for structure ---\n",
        "    # Using XML tags helps the model differentiate between instructions, examples, and the task.[3]\n",
        "    prompt = f\"\"\"\n",
        "<Role_and_Task_Definition>\n",
        "You are an expert qualitative research analyst. Your task is to determine if the code '{code_entry['Code_Name']}' should be applied to the text provided in the <Text_to_Analyze> section.\n",
        "You must first provide a step-by-step reasoning for your decision within the 'reasoning' field of a JSON object. After your reasoning, you must make a final binary decision ('APPLIES' or 'DOES NOT APPLY') in the 'decision' field.\n",
        "</Role_and_Task_Definition>\n",
        "\n",
        "<Codebook_Information>\n",
        "<Code_Name>{code_entry['Code_Name']}</Code_Name>\n",
        "<Simple_Definition>{code_entry}</Simple_Definition>\n",
        "<Inclusion_Criteria>\n",
        "{inclusion_criteria_str}\n",
        "</Inclusion_Criteria>\n",
        "<Exclusion_Criteria>\n",
        "{exclusion_criteria_str}\n",
        "</Exclusion_Criteria>\n",
        "</Codebook_Information>\n",
        "\n",
        "<Few-Shot_Exemplars>\n",
        "<Positive_Example>\n",
        "<Input_Text>{code_entry}</Input_Text>\n",
        "<Output>\n",
        "{{\n",
        "  \"reasoning\": \"{positive_reasoning}\",\n",
        "  \"decision\": \"APPLIES\"\n",
        "}}\n",
        "</Output>\n",
        "</Positive_Example>\n",
        "\n",
        "<Negative_Example>\n",
        "<Input_Text>{code_entry}</Input_Text>\n",
        "<Output>\n",
        "{{\n",
        "  \"reasoning\": \"{negative_reasoning}\",\n",
        "  \"decision\": \"DOES NOT APPLY\"\n",
        "}}\n",
        "</Output>\n",
        "</Negative_Example>\n",
        "</Few-Shot_Exemplars>\n",
        "\n",
        "<Input_Data_and_Output_Schema>\n",
        "<Text_to_Analyze>\n",
        "{text_to_analyze}\n",
        "</Text_to_Analyze>\n",
        "<Final_Output>\n",
        "Provide your final answer in a single, valid JSON object using the exact format shown in the examples above.\n",
        "</Final_Output>\n",
        "</Input_Data_and_Output_Schema>\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "hKSLpDo2ZdWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative 1\n",
        "def construct_simple_prompt(text_to_analyze, code_entry):\n",
        "    \"\"\"\n",
        "    Constructs a simplified, zero-shot prompt that asks for a direct JSON output\n",
        "    without complex examples or pre-defined reasoning steps.\n",
        "\n",
        "    Args:\n",
        "        text_to_analyze (str): The participant's text to be coded.\n",
        "        code_entry (dict): A dictionary from the llm_codebook for a single code.\n",
        "\n",
        "    Returns:\n",
        "        str: A simplified, formatted prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the criteria lists into bulleted strings\n",
        "    inclusion_criteria_str = \"\\n\".join([f\"- {item}\" for item in code_entry['Inclusion_Criteria']])\n",
        "    exclusion_criteria_str = \"\\n\".join([f\"- {item}\" for item in code_entry['Exclusion_Criteria']])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert qualitative research analyst. Your task is to determine if the code '{code_entry['Code_Name']}' should be applied to the text provided below.\n",
        "\n",
        "<Codebook_Information>\n",
        "<Code_Name>{code_entry['Code_Name']}</Code_Name>\n",
        "<Simple_Definition>{code_entry}</Simple_Definition>\n",
        "<Inclusion_Criteria>\n",
        "{inclusion_criteria_str}\n",
        "</Inclusion_Criteria>\n",
        "<Exclusion_Criteria>\n",
        "{exclusion_criteria_str}\n",
        "</Exclusion_Criteria>\n",
        "</Codebook_Information>\n",
        "\n",
        "<Text_to_Analyze>\n",
        "{text_to_analyze}\n",
        "</Text_to_Analyze>\n",
        "\n",
        "Based on the codebook and the text, provide your final answer in a single, valid JSON object with one key, \"decision\", which must have a value of either \"APPLIES\" or \"DOES NOT APPLY\". Do not add any other text before or after the JSON object.\n",
        "\n",
        "Your output must be only the JSON object.\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "FqTGdB73mVxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM inferencing"
      ],
      "metadata": {
        "id": "HZlVPHLaKSII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBLEM\n",
        "def get_llm_classification(prompt, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the LLM and gets a classification decision.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The detailed prompt for the model.\n",
        "        model: The loaded Hugging Face model.\n",
        "        tokenizer: The loaded Hugging Face tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the 'decision' and 'reasoning', or an error.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt and send it to the GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Get the length of the input tokens\n",
        "    input_length = inputs.input_ids.shape[-1]\n",
        "\n",
        "    # Generate a response from the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,  # Limit the length of the generated response\n",
        "        temperature=0.1      # Use a low temperature for more deterministic, less creative output\n",
        "    )\n",
        "\n",
        "    # Decode the output and skip the prompt part\n",
        "    # Extract only the newly generated tokens after the input prompt\n",
        "    response_text = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # --- Robust JSON Parsing ---\n",
        "    # The model's output is a string; we need to find and parse the JSON within it.\n",
        "    try:\n",
        "        # Find the start of the JSON object\n",
        "        json_start_index = response_text.find('{')\n",
        "        # Find the end of the JSON object\n",
        "        json_end_index = response_text.rfind('}') + 1\n",
        "\n",
        "        if json_start_index!= -1 and json_end_index!= -1:\n",
        "            json_string = response_text[json_start_index:json_end_index]\n",
        "            return json.loads(json_string)\n",
        "        else:\n",
        "            return {\"decision\": \"PARSE_ERROR\", \"reasoning\": response_text}\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"decision\": \"JSON_DECODE_ERROR\", \"reasoning\": response_text}"
      ],
      "metadata": {
        "id": "eW-qf8ZHZjaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your output path once at the beginning\n",
        "# p_amount = int(len(blank_df))\n",
        "# csv_name = (f\"{p_amount}_{model_name[:4]}_v2\")\n",
        "output_path = f\"/content/legaldetainment/RESULTS/simpletest.csv\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "# The main loop iterates through each participant (row) in your blank_df\n",
        "for index, row in blank_df.iterrows():\n",
        "    participant_id = row['Pno']\n",
        "    text_to_analyze = row['Qual']\n",
        "\n",
        "    # The inner loop processes each code for the current participant\n",
        "    for code_entry in llm_codebook:\n",
        "        code_name = code_entry['Code_Name']\n",
        "\n",
        "        # 1. Construct the prompt\n",
        "        prompt = construct_simple_prompt(text_to_analyze, code_entry)\n",
        "\n",
        "        # 2. Get the LLM's classification\n",
        "        llm_output = get_llm_classification(prompt, model, tokenizer)\n",
        "\n",
        "        # 3. Append the result dictionary to your list (this is very fast)\n",
        "        results.append({\n",
        "            'Pno': participant_id,\n",
        "            'code_name': code_name,\n",
        "            'llm_decision': llm_output.get('decision', 'ERROR'),\n",
        "            'llm_reasoning': llm_output.get('reasoning', 'ERROR')\n",
        "            #... add other fields you want to save...\n",
        "        })\n",
        "\n",
        "    # --- INCREMENTAL SAVE STEP ---\n",
        "    # After all codes for the current participant are processed, save all results so far.\n",
        "    # This overwrites the file with the most up-to-date results.\n",
        "    output_df = pd.DataFrame(results)\n",
        "    output_df.to_csv(output_path, index=False)\n",
        "\n",
        "    # Optional: Print a progress update\n",
        "    print(f\"‚úÖ Progress saved. Completed participant {participant_id} ({index + 1}/{len(blank_df)}).\")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "print(f\"\\n‚úÖ Full classification completed in {elapsed:.2f} seconds.\")\n",
        "print(f\"‚úîÔ∏è Final results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "TPRLRY3SvWVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output cleaned"
      ],
      "metadata": {
        "id": "AL4O-Aa4fcKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- 1. Define the path to your source CSV file ---\n",
        "csv_path = \"/content/legaldetainment/RESULTS/simpletest.csv\"\n",
        "\n",
        "try:\n",
        "    # --- 2. Read the CSV into a pandas DataFrame ---\n",
        "    results_df = pd.read_csv(csv_path)\n",
        "\n",
        "    # --- DIAGNOSTIC STEP: Print the actual column names ---\n",
        "    print(\"Actual columns found in your CSV file:\")\n",
        "    print(results_df.columns.tolist())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    # --- END DIAGNOSTIC ---\n",
        "\n",
        "    # --- 3. Select the columns you want for your clean output ---\n",
        "    # Based on the error, the column is likely named 'llm_decision'.\n",
        "    # We will select the four columns you wanted in your final table.\n",
        "    columns_to_keep = ['Pno', 'code_name', 'llm_decision', 'llm_reasoning']\n",
        "\n",
        "    # We will now create the clean DataFrame using the corrected column names.\n",
        "    clean_df = results_df[columns_to_keep]\n",
        "\n",
        "    print(\"Created a clean DataFrame with the desired columns:\")\n",
        "    display(clean_df.head())\n",
        "\n",
        "    # --- 4. Save the clean DataFrame to a new CSV file ---\n",
        "    clean_csv_path = \"/content/legaldetainment/RESULTS/clean_simpletest.csv\"\n",
        "    clean_df.to_csv(clean_csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully saved the clean data to: {clean_csv_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at '{csv_path}'. Please double-check the file name and path.\")\n",
        "except KeyError as e:\n",
        "    print(f\"ERROR: A column was not found. Please check the 'Actual columns' list above and ensure the names in 'columns_to_keep' are correct. Details: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "I9HYxsx4ifQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Step 1: Define the Robust JSON Extraction Function ---\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"\n",
        "    Finds and parses the first valid JSON object within a string.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string that may contain a JSON object.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed JSON object, or None if no valid JSON is found.\n",
        "    \"\"\"\n",
        "    # This regex pattern looks for a string starting with { and ending with }\n",
        "    # It handles nested braces and is non-greedy.\n",
        "    match = re.search(r'\\{.*\\}', str(text), re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        json_string = match.group(0)\n",
        "        try:\n",
        "            return json.loads(json_string)\n",
        "        except json.JSONDecodeError:\n",
        "            return None # Found something that looks like JSON, but it's invalid\n",
        "    return None # No JSON-like string found\n",
        "\n",
        "\n",
        "# --- Step 2: Load Your Saved Data and Apply the Salvage Operation ---\n",
        "\n",
        "# ‚ö†Ô∏è UPDATE THIS PATH to your actual saved CSV file\n",
        "csv_path = \"/content/legaldetainment/RESULTS/salvaged.csv\"\n",
        "results_df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"Re-processing saved results with robust JSON extractor...\")\n",
        "\n",
        "# Apply the new extractor to the 'llm_reasoning' column to find the JSON\n",
        "salvaged_data = []\n",
        "for index, row in results_df.iterrows():\n",
        "    parsed_json = extract_json_from_text(row['llm_reasoning'])\n",
        "    # Check if JSON was found and if it contains the 'decision' key\n",
        "    if parsed_json and 'decision' in parsed_json:\n",
        "        salvaged_data.append({\n",
        "            'Pno': row['Pno'],\n",
        "            'code_name': row['code_name'],\n",
        "            'llm_decision_raw': parsed_json['decision'],\n",
        "            'human_code': row['human_code']\n",
        "        })\n",
        "\n",
        "if not salvaged_data:\n",
        "    print(\"\\nCould not salvage any valid JSON from the saved results. This indicates a deeper issue with the model's output format.\")\n",
        "else:\n",
        "    salvaged_df = pd.DataFrame(salvaged_data)\n",
        "\n",
        "    print(f\"\\nSuccessfully salvaged {len(salvaged_df)} classifications!\")\n",
        "\n",
        "    # --- Step 3: Run the Evaluation on the SALVAGED Data ---\n",
        "\n",
        "    # Convert 'APPLIES'/'DOES NOT APPLY' to 1/0\n",
        "    salvaged_df['llm_decision_numeric'] = salvaged_df['llm_decision_raw'].apply(lambda x: 1 if x == 'APPLIES' else 0)\n",
        "\n",
        "    # Reshape to \"wide\" format to match humandata\n",
        "    model_coded_wide_df = salvaged_df.pivot(index='Pno', columns='code_name', values='llm_decision_numeric')\n",
        "    model_coded_wide_df.reset_index(inplace=True)\n",
        "    model_coded_wide_df = model_coded_wide_df.rename_axis(None, axis=1)\n",
        "\n",
        "    # Merge with Human Data (assuming 'humandata' is loaded and has \"Yes\"/\"No\")\n",
        "    humandata_numeric = humandata.replace({\"Yes\": 1, \"No\": 0})\n",
        "    eval_df = pd.merge(humandata_numeric, model_coded_wide_df, on='Pno', suffixes=('_human', '_model'))\n",
        "\n",
        "    # --- Step 4: Calculate and Print Performance Metrics ---\n",
        "    all_human_labels = []\n",
        "    all_model_labels = []\n",
        "    code_columns = [col for col in model_coded_wide_df.columns if col!= 'Pno']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Per-Code Performance Metrics (on SALVAGED results):\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for code in code_columns:\n",
        "        human_col = f\"{code}_human\"\n",
        "        model_col = f\"{code}_model\"\n",
        "        if human_col in eval_df.columns and model_col in eval_df.columns:\n",
        "            valid_evals = eval_df[[human_col, model_col]].dropna()\n",
        "            all_human_labels.extend(valid_evals[human_col].tolist())\n",
        "            all_model_labels.extend(valid_evals[model_col].tolist())\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                valid_evals[human_col], valid_evals[model_col],\n",
        "                average='binary', pos_label=1, zero_division=0\n",
        "            )\n",
        "            print(f\"\\nMetrics for code: '{code}'\")\n",
        "            print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Overall Performance Metrics (on SALVAGED results):\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if all_human_labels:\n",
        "        overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
        "            all_human_labels, all_model_labels,\n",
        "            average='binary', pos_label=1, zero_division=0\n",
        "        )\n",
        "        print(f\"Overall Precision: {overall_precision:.3f}\")\n",
        "        print(f\"Overall Recall:    {overall_recall:.3f}\")\n",
        "        print(f\"Overall F1-Score:  {overall_f1:.3f}\")\n",
        "    else:\n",
        "        print(\"No valid data to calculate overall metrics.\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "oN43FDZ-kyZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance?"
      ],
      "metadata": {
        "id": "gn1JXtXBfhua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Load Your Saved Data ---\n",
        "# Make sure to update this path to your actual CSV file\n",
        "csv_path = \"/content/legaldetainment/RESULTS/latesttest.csv\"\n",
        "results_df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Loaded {len(results_df)} total classifications.\")\n",
        "\n",
        "# --- 1. Filter out the rows where the model failed to produce valid JSON ---\n",
        "successful_results_df = results_df[~results_df['llm_decision_raw'].isin()].copy()\n",
        "error_count = len(results_df) - len(successful_results_df)\n",
        "print(f\"Found {error_count} errors ({error_count/len(results_df):.1%}). Analyzing {len(successful_results_df)} successful classifications.\\n\")\n",
        "\n",
        "# Convert 'APPLIES'/'DOES NOT APPLY' to 1/0 for the model's decision\n",
        "successful_results_df['llm_decision_numeric'] = successful_results_df['llm_decision_raw'].apply(lambda x: 1 if x == 'APPLIES' else 0)\n",
        "\n",
        "\n",
        "# --- 2. Reshape the data from \"long\" to \"wide\" format ---\n",
        "# This pivots the data so each row is one participant and each column is a code, matching your humandata.\n",
        "model_coded_wide_df = successful_results_df.pivot(index='Pno', columns='code_name', values='llm_decision_numeric')\n",
        "model_coded_wide_df.reset_index(inplace=True)\n",
        "model_coded_wide_df = model_coded_wide_df.rename_axis(None, axis=1)\n",
        "\n",
        "print(\"Model results reshaped into wide format:\")\n",
        "display(model_coded_wide_df.head())\n",
        "\n",
        "\n",
        "# --- 3. Merge with Human Data and Evaluate ---\n",
        "# Make sure your 'humandata' DataFrame is loaded and formatted (1s and 0s)\n",
        "# This assumes 'humandata' has 1/0 for codes, not \"Yes\"/\"No\"\n",
        "humandata_numeric = humandata.replace({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Merge the dataframes on the participant number\n",
        "eval_df = pd.merge(\n",
        "    humandata_numeric,\n",
        "    model_coded_wide_df,\n",
        "    on='Pno',\n",
        "    suffixes=('_human', '_model')\n",
        ")\n",
        "\n",
        "print(\"\\nMerged data for evaluation:\")\n",
        "display(eval_df.head())\n",
        "\n",
        "\n",
        "# --- 4. Calculate and Print Performance Metrics ---\n",
        "all_human_labels = []\n",
        "all_model_labels = []\n",
        "\n",
        "# Get a list of the code columns we are evaluating\n",
        "code_columns = [col for col in model_coded_wide_df.columns if col!= 'Pno']\n",
        "\n",
        "for code in code_columns:\n",
        "    human_col = f\"{code}_human\"\n",
        "    model_col = f\"{code}_model\"\n",
        "\n",
        "    # Ensure columns exist before trying to access them\n",
        "    if human_col in eval_df.columns and model_col in eval_df.columns:\n",
        "        # Drop rows where the model didn't have a prediction (NaN)\n",
        "        valid_evals = eval_df[[human_col, model_col]].dropna()\n",
        "\n",
        "        all_human_labels.extend(valid_evals[human_col].tolist())\n",
        "        all_model_labels.extend(valid_evals[model_col].tolist())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Overall Performance Metrics (on successfully decoded results):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate overall metrics\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_human_labels,\n",
        "    all_model_labels,\n",
        "    average='binary', # Use 'binary' for 1/0 labels\n",
        "    pos_label=1,\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(f\"Overall Precision: {precision:.3f}\")\n",
        "print(f\"Overall Recall:    {recall:.3f}\")\n",
        "print(f\"Overall F1-Score:  {f1:.3f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "E_Z2kj7DfjcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõü SAVE HERE üõü"
      ],
      "metadata": {
        "id": "iu_itx9KxWXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Access Your GitHub Token from Secrets ---\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get your credentials from Colab's secret manager\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "github_user = \"ninenine-9\" # Your GitHub username\n",
        "repo_name = \"legaldetainment\"\n",
        "\n",
        "# Construct the secure remote URL\n",
        "remote_url = f\"https://{github_user}:{github_token}@github.com/{github_user}/{repo_name}.git\"\n",
        "\n",
        "# Set the remote URL for this push operation\n",
        "!git remote set-url origin {remote_url}\n",
        "\n",
        "\n",
        "# --- Step 3: Configure Git, Add, Commit, and Push ---\n",
        "!git config --global user.email \"nine.adler.20@ucl.ac.uk\"\n",
        "!git config --global user.name {github_user}\n",
        "\n",
        "# Add all changed files (with the corrected space)\n",
        "print(\"\\nStaging files...\")\n",
        "!git add .\n",
        "\n",
        "# Commit the changes\n",
        "print(\"\\nCommitting files...\")\n",
        "# Using -m flag for the message. Use \"git status\" to see if there's anything to commit.\n",
        "!git commit -m \"Updated script and results from Colab session\"\n",
        "\n",
        "# Push the changes using the authenticated URL\n",
        "print(\"\\nPushing to GitHub...\")\n",
        "!git push origin main"
      ],
      "metadata": {
        "id": "uySJXfkeo1Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in case of emergency"
      ],
      "metadata": {
        "id": "ekNtOFHzJRxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADD THIS CODE TO MAKE YOUR SCRIPT RESUMABLE ---\n",
        "\n",
        "# Define the output path (same as before)\n",
        "output_path = f\"/content/legaldetainment/RESULTS/{csv_name}.csv\"\n",
        "\n",
        "processed_pnos = set()\n",
        "\n",
        "# Check if the results file already exists and has content\n",
        "if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "    try:\n",
        "        existing_results_df = pd.read_csv(output_path)\n",
        "        if 'Pno' in existing_results_df.columns:\n",
        "            processed_pnos = set(existing_results_df['Pno'].unique())\n",
        "            print(f\"Found {len(processed_pnos)} participants already processed. Resuming from where we left off.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Results file is empty. Starting from the beginning.\")\n",
        "\n",
        "# --- Your main loop starts here ---\n",
        "# The only change is to filter the dataframe you are iterating over\n",
        "\n",
        "# Filter blank_df to only include participants that have NOT been processed\n",
        "unprocessed_df = blank_df[~blank_df['Pno'].isin(processed_pnos)]\n",
        "print(f\"Starting classification for {len(unprocessed_df)} remaining participants.\")\n",
        "\n",
        "# Now, iterate over the UNPROCESSED data\n",
        "for index, row in unprocessed_df.iterrows():\n",
        "    #... the rest of your loop code remains exactly the same...\n",
        "    #... it will automatically save to the same CSV file, appending the new results..."
      ],
      "metadata": {
        "id": "-Vsoy1XDJTo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}