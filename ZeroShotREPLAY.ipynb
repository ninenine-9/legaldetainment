{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ6DaeYcVsp/jiddcAAIML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninenine-9/legaldetainment/blob/main/ZeroShotREPLAY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "coming from:\n",
        "https://colab.research.google.com/drive/1ZIC8xMktkdi_i09_62PBeS1zipveqM2x?authuser=1#scrollTo=eJxDc8FRk2C9&line=18&uniqifier=1\n",
        "\n",
        "Adapting to Stephen's legal detainment exp"
      ],
      "metadata": {
        "id": "J6U_dOhQm7g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0Ô∏è‚É£ Schema"
      ],
      "metadata": {
        "id": "qejxrEfonOd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B7_ksMKmwUn",
        "outputId": "cfad4cfd-2ee3-4eb1-bca1-e78a3789501b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Schema, Columns & Labels done\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Literal\n",
        "\n",
        "class CodedResponse(BaseModel):\n",
        "    GuiltMoreLikely: Literal[\"Yes\", \"No\"]\n",
        "    GuiltLessLikely: Literal[\"Yes\", \"No\"]\n",
        "    NoInformation_Evidence: Literal[\"Yes\", \"No\"]\n",
        "    InnocentUntilProvenGuilty: Literal[\"Yes\", \"No\"]\n",
        "    Confound: Literal[\"Yes\", \"No\"]\n",
        "    Unclassified_Other: Literal[\"Yes\", \"No\"]\n",
        "\n",
        "binary_columns = [\n",
        "        \"GuiltMoreLikely\",\n",
        "        \"GuiltLessLikely\",\n",
        "        \"NoInformation_Evidence\",\n",
        "        \"InnocentUntilProvenGuilty\",\n",
        "        \"Confound\",\n",
        "        \"Unclassified_Other\"\n",
        "    ]\n",
        "\n",
        "binary_labels = [\"Yes\", \"No\"]\n",
        "\n",
        "# Define which codes are in the mutually exclusive cluster\n",
        "cluster_codes = [\"GuiltMoreLikely\", \"GuiltLessLikely\", \"NoInformation_Evidence\"]\n",
        "independent_codes = [\"InnocentUntilProvenGuilty\", \"Confound\"]\n",
        "all_codes = cluster_codes + independent_codes + [\"Unclassified_Other\"]\n",
        "\n",
        "print(\"‚úîÔ∏è Schema, Columns & Labels done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1Ô∏è‚É£ Imports"
      ],
      "metadata": {
        "id": "WY0M7NNcnS3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from google.colab import drive\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Literal\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score # üÜï NEW\n",
        "\n",
        "print(\"‚úîÔ∏è Imports done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NfeqkqjnhCg",
        "outputId": "ced86327-f97e-4f86-a382-3bc7d925a2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Imports done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2Ô∏è‚É£ Data loading"
      ],
      "metadata": {
        "id": "a0XsZw8OoA4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "SAsDFdtAvErl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/ninenine-9/legaldetainment.git\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!cp -r /content/legaldetainment /content/drive/MyDrive/\n",
        "\n",
        "print(\"‚úîÔ∏è Git Repository successfully cloned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqLsvyzyl4k0",
        "outputId": "469bd52f-9d7e-493b-f153-84fe25e0ec0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'legaldetainment' already exists and is not an empty directory.\n",
            "Mounted at /content/drive\n",
            "‚úîÔ∏è Git Repository successfully cloned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading\n",
        "`/content/legaldetainment/DATA/legaldetainment_humandata.xlsx` contains our human-coded data. Below is an explanation of the spreadsheet's various sheets:\n",
        "- `data` = the full dataset (minus `removed`)\n",
        "- `removed` = participants I've removed because of inconclusive coding\n",
        "- `ALL(10)` = a sample of 10 participants\n",
        "- `30participants` = a sample of 30 participants\n",
        "\n",
        "`/content/legaldetainment/DATA/legaldetainment_blankdata.xlsx` contains our blank data. It contains Pno, Qual and the relevant column headers for the codes (but the content of these columns is blank). Below is an explanation of the spreadsheet's various sheets:\n",
        "- `ALL` = the full dataset (minus `removed`)\n",
        "- `ALL(10)` = a sample of 10 participants\n",
        "- `ALL(20)` = a sample of 20 participants\n",
        "- `30participants` = a sample of 30 participants\n",
        "- etc..\n",
        "\n",
        "‚ö†Ô∏è YOU SHOULD ALWAYS HAVE MATCHING SHEETS BETWEEN THE BLANK DATASET AND THE HUMAN-CODED DATA!\n",
        "For example, if you create a new tab/sheet for testing 40 participants, you must have a sheet in both the full dataset and human-coded data with corresponding participants (Pno).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "_example_\n",
        "\n",
        "`legaldetainment_347p_ALL.xlsx, sheet = ALL(3)`\n",
        "| Pno | Qual |\n",
        "|:-----------|:------------:|\n",
        "| 200 | Lorem ipsum  |\n",
        "| 35 | dolor sit amet  |\n",
        "| 298 | Duis aute irure  |\n",
        "\n",
        "`legaldetainment_blankdata.xlsx, sheet = ALL(3)`\n",
        "| Pno | Qual |\n",
        "|:-----------|:------------:|\n",
        "| 200 | Lorem ipsum  |\n",
        "| 35 | dolor sit amet  |\n",
        "| 298 | Duis aute irure  |\n",
        "\n",
        "`Same Pnos in both datasets ‚úÖ`\n",
        "\n",
        "\n",
        "‚ùî Still unsure? the last block of this section can check this for you\n"
      ],
      "metadata": {
        "id": "n0rcmTnKBtld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Materials"
      ],
      "metadata": {
        "id": "e_rkxhPdwDq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/legaldetainment/INPUTS/legaldetainment_story.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "    study_context = f.read()\n",
        "\n",
        "\n",
        "with open(\"/content/legaldetainment/INPUTS/legaldetainment_codinginstructions.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "    coding_manual = f.read()"
      ],
      "metadata": {
        "id": "aU6iwNeuwJEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blank dataset"
      ],
      "metadata": {
        "id": "qwo1I5TEvzPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/legaldetainment/DATA/legaldetainment_blankdata.xlsx\",\n",
        "                     sheet_name = '30participants') # üìã for cell H & G\n",
        "\n",
        "data = data.dropna(subset=[\"Pno\"])\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "print(f\"Dataset size: {data.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTGqYlkdB9ES",
        "outputId": "53021bba-b728-4b9d-f4bb-c3d3490e6523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: (30, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human-coded (comparison) dataset"
      ],
      "metadata": {
        "id": "NuM1nBhXv34y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "humandata = pd.read_excel(\"/content/legaldetainment/DATA/legaldetainment_humandata.xlsx\", sheet_name = '30participants') # üìã for cell I\n",
        "\n",
        "humandata = humandata.dropna(subset=[\"Pno\"])\n",
        "humandata = humandata.replace({0: \"No\", 1: \"Yes\", \"0\": \"No\", \"1\": \"Yes\"})\n",
        "humandata = humandata.rename(columns={\n",
        "    \"NoInformation/Evidence\": \"NoInformation_Evidence\",\n",
        "    \"Unclassified/Other\": \"Unclassified_Other\"\n",
        "})\n",
        "humandata.columns = humandata.columns.str.strip()\n",
        "\n",
        "print(\"‚úîÔ∏è Data loading done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y67nG9a_wAzd",
        "outputId": "91251907-c515-4077-abde-207c610b7235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Data loading done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3Ô∏è‚É£ Model loading"
      ],
      "metadata": {
        "id": "QrvztCg5oJ8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "CONFIDENCE_THRESHOLD = 0.1 # ‚úèÔ∏è Higher = more conservative (more blanks). Lower = more guesses\n",
        "\n",
        "\n",
        "# ‚ÑπÔ∏è ‚úèÔ∏è model dependent (see line 20-21, must match)\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\",\n",
        "                      device=device)\n",
        "print(\"‚úîÔ∏è Model loading done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9PuAAf6tokK",
        "outputId": "3f932d81-aa55-4132-bf11-6bb872852692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Model loading done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4Ô∏è‚É£ Model running"
      ],
      "metadata": {
        "id": "3oQJ3pvfoQJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "v2 problem\n",
        "your threshold logic is working for the other columns, but for Unclassified_Other the post-processing rule dominates, which is why changing the threshold does not change final counts for that code.\n",
        "\n",
        "If you want threshold changes to affect Unclassified_Other, you‚Äôd need to modify or remove that exclusivity/post-processing rule."
      ],
      "metadata": {
        "id": "SJPbEDB587zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "# ‚ûã Loop through participant responses\n",
        "for i, row in data.iterrows():\n",
        "    text = row[\"Qual\"]\n",
        "    result_row = row.to_dict()\n",
        "\n",
        "    # Handle empty or missing text\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        for col in binary_columns:\n",
        "            result_row[col] = \"No\"\n",
        "            result_row[f\"{col}_score\"] = 0.0 # üÜï NEW (we're now recording the model's score)\n",
        "        results.append(result_row)\n",
        "        continue\n",
        "\n",
        "    # ‚ûç Classify binary columns\n",
        "    scores_record = {}\n",
        "    for col in binary_columns:\n",
        "        response = classifier(text, candidate_labels=[\"Yes\", \"No\"])\n",
        "        best_label = response[\"labels\"][0]\n",
        "        best_score = response[\"scores\"][0]\n",
        "        result_row[col] = best_label if best_score >= CONFIDENCE_THRESHOLD else \"No\"\n",
        "\n",
        "\n",
        "        result_row[f\"{col}_score_Yes\"] = response[\"scores\"][response[\"labels\"].index(\"Yes\")]\n",
        "        result_row[f\"{col}_score_No\"] = response[\"scores\"][response[\"labels\"].index(\"No\")]\n",
        "        scores_record[col] = result_row[f\"{col}_score_Yes\"]\n",
        "\n",
        "        # scores_record[col] = best_score\n",
        "\n",
        "    # ‚ûé Enforce exclusivity within cluster codes\n",
        "    yes_cluster = [c for c in cluster_codes if result_row[c] == \"Yes\"]\n",
        "\n",
        "    if len(yes_cluster) > 1:\n",
        "        # Keep only the one with highest confidence\n",
        "        best = max(yes_cluster, key=lambda c: scores_record[c])\n",
        "        for c in cluster_codes:\n",
        "            result_row[c] = \"Yes\" if c == best else \"No\"\n",
        "\n",
        "    # ‚ûè Enforce Unclassified_Other rule\n",
        "    if all(result_row[c] == \"No\" for c in cluster_codes + independent_codes): # ‚ö†Ô∏è might have a pronlem here\n",
        "        result_row[\"Unclassified_Other\"] = \"Yes\"\n",
        "    else:\n",
        "        result_row[\"Unclassified_Other\"] = \"No\"\n",
        "\n",
        "    results.append(result_row)\n",
        "    # print(response)\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(data)} responses...\") # ‚ÑπÔ∏è keeps us posted on progress\n",
        "\n",
        "scores = [r[f\"{col}_score_Yes\"] for r in results]\n",
        "print(min(scores), max(scores), np.mean(scores))\n",
        "sum_yes = sum(row[col] == \"Yes\" for row in results)\n",
        "print(f\"{col}: {sum_yes} Yes at threshold {CONFIDENCE_THRESHOLD}\")\n",
        "\n",
        "\n",
        "# ‚ûé Timer end\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "avg_time = elapsed / len(data)\n",
        "print(f\"\\n‚úÖ Classification completed in {elapsed:.2f} seconds.\")\n",
        "print(f\"Average time per entry: {avg_time:.3f} seconds.\")\n",
        "\n",
        "print(\"‚úîÔ∏è Running completed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3026223-a6b6-4773-9eed-4b03e5d79ca5",
        "id": "5y4dnUV786y7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10/30 responses...\n",
            "Processed 20/30 responses...\n",
            "Processed 30/30 responses...\n",
            "0.0648532509803772 0.4666767120361328 0.24034789924820263\n",
            "Unclassified_Other: 30 Yes at threshold 0.1\n",
            "\n",
            "‚úÖ Classification completed in 302.91 seconds.\n",
            "Average time per entry: 10.097 seconds.\n",
            "‚úîÔ∏è Running completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1 ‚Äì Keep Unclassified_Other as a fallback but respect threshold"
      ],
      "metadata": {
        "id": "tBQ6PP1qyY6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "# ‚ûã Loop through participant responses\n",
        "for i, row in data.iterrows():\n",
        "    text = row[\"Qual\"]\n",
        "    result_row = row.to_dict()\n",
        "\n",
        "    # Handle empty or missing text\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        for col in binary_columns:\n",
        "            result_row[col] = \"No\"\n",
        "            result_row[f\"{col}_score_Yes\"] = 0.0\n",
        "            result_row[f\"{col}_score_No\"] = 0.0\n",
        "        results.append(result_row)\n",
        "        continue\n",
        "\n",
        "    # Store Yes scores for exclusivity logic\n",
        "    scores_record = {}\n",
        "\n",
        "    # Classify all columns except Unclassified_Other first\n",
        "    for col in binary_columns:\n",
        "        if col == \"Unclassified_Other\":\n",
        "            continue  # handle separately\n",
        "        response = classifier(text, candidate_labels=[\"Yes\", \"No\"])\n",
        "        if isinstance(response, list):\n",
        "            response = response[0]\n",
        "        best_label = response[\"labels\"][0]\n",
        "        best_score = response[\"scores\"][0]\n",
        "\n",
        "        # Assign label based on threshold\n",
        "        result_row[col] = best_label if best_score >= CONFIDENCE_THRESHOLD else \"No\"\n",
        "\n",
        "        # Store individual scores\n",
        "        result_row[f\"{col}_score_Yes\"] = response[\"scores\"][response[\"labels\"].index(\"Yes\")]\n",
        "        result_row[f\"{col}_score_No\"] = response[\"scores\"][response[\"labels\"].index(\"No\")]\n",
        "        scores_record[col] = result_row[f\"{col}_score_Yes\"]\n",
        "\n",
        "    # Enforce exclusivity within cluster_codes\n",
        "    yes_cluster = [c for c in cluster_codes if result_row[c] == \"Yes\"]\n",
        "    if len(yes_cluster) > 1:\n",
        "        # Keep only the one with highest confidence\n",
        "        best = max(yes_cluster, key=lambda c: scores_record[c])\n",
        "        for c in cluster_codes:\n",
        "            result_row[c] = \"Yes\" if c == best else \"No\"\n",
        "\n",
        "    # Classify Unclassified_Other with threshold\n",
        "    col = \"Unclassified_Other\"\n",
        "    response = classifier(text, candidate_labels=[\"Yes\", \"No\"])\n",
        "    if isinstance(response, list):\n",
        "        response = response[0]\n",
        "    score_yes = response[\"scores\"][response[\"labels\"].index(\"Yes\")]\n",
        "    score_no = response[\"scores\"][response[\"labels\"].index(\"No\")]\n",
        "    result_row[col] = \"Yes\" if score_yes >= CONFIDENCE_THRESHOLD else \"No\"\n",
        "    result_row[f\"{col}_score_Yes\"] = score_yes\n",
        "    result_row[f\"{col}_score_No\"] = score_no\n",
        "\n",
        "    # Final fallback rule for Unclassified_Other\n",
        "    if all(result_row[c] == \"No\" for c in cluster_codes + independent_codes + [col]):\n",
        "        result_row[col] = \"Yes\"\n",
        "\n",
        "    results.append(result_row)\n",
        "\n",
        "scores = [r[f\"{col}_score_Yes\"] for r in results]\n",
        "print(min(scores), max(scores), np.mean(scores))\n",
        "sum_yes = sum(row[col] == \"Yes\" for row in results)\n",
        "print(f\"{col}: {sum_yes} Yes at threshold {CONFIDENCE_THRESHOLD}\")\n",
        "\n",
        "\n",
        "# ‚ûé Timer end\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "avg_time = elapsed / len(data)\n",
        "print(f\"\\n‚úÖ Classification completed in {elapsed:.2f} seconds.\")\n",
        "print(f\"Average time per entry: {avg_time:.3f} seconds.\")\n",
        "\n",
        "print(\"‚úîÔ∏è Running completed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06099fa8-5bc4-4324-bdfa-f5902533df97",
        "id": "0HA6x5CYyWQR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0648532509803772 0.4666767120361328 0.24034789924820263\n",
            "Unclassified_Other: 30 Yes at threshold 0.1\n",
            "\n",
            "‚úÖ Classification completed in 319.06 seconds.\n",
            "Average time per entry: 10.635 seconds.\n",
            "‚úîÔ∏è Running completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classifying independent codes first"
      ],
      "metadata": {
        "id": "fvCl-EHS1LR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in data.iterrows():\n",
        "    text = row[\"Qual\"]\n",
        "    result_row = row.to_dict()\n",
        "\n",
        "    # Handle empty or missing text\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        for col in binary_columns:\n",
        "            result_row[col] = \"No\"\n",
        "            result_row[f\"{col}_score_Yes\"] = 0.0\n",
        "            result_row[f\"{col}_score_No\"] = 0.0\n",
        "        results.append(result_row)\n",
        "        continue\n",
        "\n",
        "    scores_record = {}\n",
        "\n",
        "    # 1Ô∏è‚É£ Classify cluster + independent codes first\n",
        "    for col in cluster_codes + independent_codes:\n",
        "        response = classifier(text, candidate_labels=[\"Yes\", \"No\"])\n",
        "        if isinstance(response, list):\n",
        "            response = response[0]\n",
        "\n",
        "        score_yes = response[\"scores\"][response[\"labels\"].index(\"Yes\")]\n",
        "        score_no  = response[\"scores\"][response[\"labels\"].index(\"No\")]\n",
        "\n",
        "        # Threshold-based assignment\n",
        "        result_row[col] = \"Yes\" if score_yes >= CONFIDENCE_THRESHOLD else \"No\"\n",
        "\n",
        "        # Store scores\n",
        "        result_row[f\"{col}_score_Yes\"] = score_yes\n",
        "        result_row[f\"{col}_score_No\"] = score_no\n",
        "        scores_record[col] = score_yes\n",
        "\n",
        "    # 2Ô∏è‚É£ Enforce exclusivity within cluster codes\n",
        "    yes_cluster = [c for c in cluster_codes if result_row[c] == \"Yes\"]\n",
        "    if len(yes_cluster) > 1:\n",
        "        best = max(yes_cluster, key=lambda c: scores_record[c])\n",
        "        for c in cluster_codes:\n",
        "            result_row[c] = \"Yes\" if c == best else \"No\"\n",
        "\n",
        "    # 3Ô∏è‚É£ Classify Unclassified_Other based on threshold first\n",
        "    col = \"Unclassified_Other\"\n",
        "    response = classifier(text, candidate_labels=[\"Yes\", \"No\"])\n",
        "    if isinstance(response, list):\n",
        "        response = response[0]\n",
        "    score_yes = response[\"scores\"][response[\"labels\"].index(\"Yes\")]\n",
        "    score_no  = response[\"scores\"][response[\"labels\"].index(\"No\")]\n",
        "\n",
        "    result_row[col] = \"Yes\" if score_yes >= CONFIDENCE_THRESHOLD else \"No\"\n",
        "    result_row[f\"{col}_score_Yes\"] = score_yes\n",
        "    result_row[f\"{col}_score_No\"] = score_no\n",
        "\n",
        "    # 4Ô∏è‚É£ Final fallback: only if all other codes are \"No\" after thresholding\n",
        "    if all(result_row[c] == \"No\" for c in cluster_codes + independent_codes):\n",
        "        result_row[col] = \"Yes\"\n",
        "\n",
        "    results.append(result_row)\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(data)} responses...\")\n",
        "scores = [r[f\"{col}_score_Yes\"] for r in results]\n",
        "print(min(scores), max(scores), np.mean(scores))\n",
        "sum_yes = sum(row[col] == \"Yes\" for row in results)\n",
        "print(f\"{col}: {sum_yes} Yes at threshold {CONFIDENCE_THRESHOLD}\")\n",
        "\n",
        "# Timer end\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "avg_time = elapsed / len(data)\n",
        "print(f\"\\n‚úÖ Classification completed in {elapsed:.2f} seconds.\")\n",
        "print(f\"Average time per entry: {avg_time:.3f} seconds.\")\n",
        "print(\"‚úîÔ∏è Running completed\")\n",
        "\n",
        "# Convert results to DataFrame if needed\n",
        "results_df = pd.DataFrame(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGCeUFWI01RY",
        "outputId": "7ecff768-afc6-49c4-a89b-edf37784d648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10/30 responses...\n",
            "Processed 20/30 responses...\n",
            "Processed 30/30 responses...\n",
            "0.0648532509803772 0.4666767120361328 0.24034789924820263\n",
            "Unclassified_Other: 30 Yes at threshold 0.1\n",
            "\n",
            "‚úÖ Classification completed in 308.20 seconds.\n",
            "Average time per entry: 10.273 seconds.\n",
            "‚úîÔ∏è Running completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5Ô∏è‚É£Saving outputs"
      ],
      "metadata": {
        "id": "K2cm0PyVoUvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/legaldetainment/RESULTS/30p_CF01_troubleshoot.csv\" # ‚úèÔ∏è must rename for each run (+1 for example)\n",
        "output_df = pd.DataFrame(results)\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "output_df.to_csv(output_path, index=False)\n",
        "print(f\"‚úîÔ∏è Results saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaA7f4zN1NkN",
        "outputId": "ba81ac39-ffe2-44b6-82d2-98c830341748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'The judge decided not to detain x up until their trial therefore he may have already formed the opinion that he is not guilty.', 'labels': ['No', 'Yes'], 'scores': [0.7886896729469299, 0.21131031215190887]}\n",
            "‚úîÔ∏è Results saved to /content/legaldetainment/RESULTS/30p_CF01.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6Ô∏è‚É£ Evaluation"
      ],
      "metadata": {
        "id": "XgiM4cP1oX4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Of LM vs human"
      ],
      "metadata": {
        "id": "TbT394H-Csaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# humandata = humandata.replace({'0': 'No', '1': 'Yes'})\n",
        "\n",
        "# # Clean up column names in output_df by stripping whitespace\n",
        "# output_df.columns = output_df.columns.str.strip()\n",
        "\n",
        "# # Add print statements to check column names before merge\n",
        "# print(\"Columns in humandata before merge:\")\n",
        "# print(humandata.columns)\n",
        "# print(\"\\nColumns in output_df before merge:\")\n",
        "# print(output_df.columns)\n",
        "\n",
        "# Compare human and model coding\n",
        "eval_df = humandata[[\"Pno\"] + all_codes].merge(\n",
        "    output_df[[\"Pno\"] + all_codes],\n",
        "    on=\"Pno\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Boolean: all codes match for that Pno\n",
        "eval_df[\"all_match\"] = eval_df.apply(\n",
        "    lambda row: all(row[code + \"_x\"] == row[code + \"_y\"] for code in all_codes),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Count matches\n",
        "total = len(eval_df)\n",
        "matches = eval_df[\"all_match\"].sum()\n",
        "percentage = matches / total * 100 if total else 0\n",
        "\n",
        "print(f\"\\nTotal Pnos compared: {total}\")\n",
        "print(f\"Matching on all codes: {matches} ({percentage:.1f}%)\") # üìã for cell N & O\n",
        "\n",
        "# Print Pnos that match on all codes\n",
        "matching_pnos = eval_df[eval_df['all_match'] == True]['Pno'].tolist()\n",
        "print(f\"Pnos matching on all codes: {matching_pnos}\") # üìã for cell P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isgY_CxboMcj",
        "outputId": "b10b3fa6-a4aa-42fc-ce71-b601abd0bca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Pnos compared: 30\n",
            "Matching on all codes: 14 (46.7%)\n",
            "Pnos matching on all codes: [232, 247, 223, 212, 342, 18, 89, 259, 267, 253, 299, 327, 175, 31]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Of LM performance"
      ],
      "metadata": {
        "id": "W1ayC4QbC3xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# üî∏ Redirect stdout to file\n",
        "log_file = \"/content/legaldetainment/RESULTS/30p_CF01_log_troubleshoot.txt\"\n",
        "original_stdout = sys.stdout\n",
        "with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    sys.stdout = f\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Per-Code Metrics:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for code in all_codes:\n",
        "        human_col = f\"{code}_x\"\n",
        "        model_col = f\"{code}_y\"\n",
        "\n",
        "        human_vals = eval_df[human_col]\n",
        "        model_vals = eval_df[model_col]\n",
        "\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            human_vals, model_vals,\n",
        "            labels=[\"Yes\", \"No\"],\n",
        "            average=None,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        precision_yes = precision[0]\n",
        "        recall_yes = recall[0]\n",
        "        f1_yes = f1[0]\n",
        "        support_yes = support[0]\n",
        "\n",
        "        print(f\"\\n{code}:\")\n",
        "        print(f\"  Precision: {precision_yes:.3f}\")\n",
        "        print(f\"  Recall:    {recall_yes:.3f}\")\n",
        "        print(f\"  F1-Score:  {f1_yes:.3f}\")\n",
        "        print(f\"  Support:   {support_yes} actual 'Yes' cases\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Overall Metrics:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    all_human = []\n",
        "    all_model = []\n",
        "    for code in all_codes:\n",
        "        all_human.extend(eval_df[f\"{code}_x\"].tolist())\n",
        "        all_model.extend(eval_df[f\"{code}_y\"].tolist())\n",
        "\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        all_human, all_model,\n",
        "        labels=[\"Yes\", \"No\"],\n",
        "        average=None,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"Overall Precision: {precision[0]:.3f}\")\n",
        "    print(f\"Overall Recall:    {recall[0]:.3f}\")\n",
        "    print(f\"Overall F1-Score:  {f1[0]:.3f}\")\n",
        "\n",
        "    # ‚úÖ restore stdout automatically when file closes\n",
        "    sys.stdout = original_stdout\n",
        "\n",
        "print(f\"Console output saved to: {log_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uL3XIl1gOik",
        "outputId": "319601f2-f45d-478b-e2e7-d002a9b8887f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Console output saved to: /content/legaldetainment/RESULTS/30p_CF01_log.txt\n"
          ]
        }
      ]
    }
  ]
}