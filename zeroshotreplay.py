# -*- coding: utf-8 -*-
"""ZeroShotREPLAY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKgZuvl7Q7ueN_pBuoYU_J5G1nLph2S9

coming from:
https://colab.research.google.com/drive/1ZIC8xMktkdi_i09_62PBeS1zipveqM2x?authuser=1#scrollTo=eJxDc8FRk2C9&line=18&uniqifier=1

Adapting to Stephen's legal detainment exp

# 0Ô∏è‚É£ Schema
"""

from pydantic import BaseModel
from typing import Optional, Literal

class CodedResponse(BaseModel):
    GuiltMoreLikely: Literal["Yes", "No"]
    GuiltLessLikely: Literal["Yes", "No"]
    NoInformation_Evidence: Literal["Yes", "No"]
    InnocentUntilProvenGuilty: Literal["Yes", "No"]
    Confound: Literal["Yes", "No"]
    Unclassified_Other: Literal["Yes", "No"]

binary_columns = [
        "GuiltMoreLikely",
        "GuiltLessLikely",
        "NoInformation_Evidence",
        "InnocentUntilProvenGuilty",
        "Confound",
        "Unclassified_Other"
    ]

binary_labels = ["Yes", "No"]

# Define which codes are in the mutually exclusive cluster
cluster_codes = ["GuiltMoreLikely", "GuiltLessLikely", "NoInformation_Evidence"]
independent_codes = ["InnocentUntilProvenGuilty", "Confound"]
all_codes = cluster_codes + independent_codes + ["Unclassified_Other"]

print("‚úîÔ∏è Schema, Columns & Labels done")

"""# 1Ô∏è‚É£ Imports"""

import pandas as pd
import numpy as np
import torch
import time
from google.colab import drive
from pydantic import BaseModel
from typing import Optional, Literal
import os
from transformers import pipeline

print("‚úîÔ∏è Imports done")

"""# 2Ô∏è‚É£ Data loading

## Retrieval
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/ninenine-9/legaldetainment.git

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!cp -r /content/legaldetainment /content/drive/MyDrive/

print("‚úîÔ∏è Git Repository successfully cloned")

"""## Uploading
`legaldetainment_347p_ALL.xlsx` contains our human-coded data. Below is an explanation of the spreadsheet's various sheets:
- `data` = the full dataset (minus `removed`)
- `removed` = participants I've removed because of inconclusive coding
- `ALL(10)` = a sample of 10 participants

`legaldetainment_blankdata.xlsx` contains our blank data. It contains Pno, Qual and the relevant column headers for the codes (but the content of these columns is blank). Below is an explanation of the spreadsheet's various sheets:
- `ALL` = the full dataset (minus `removed`)
- `ALL(10)` = a sample of 10 participants
- `ALL(20)` = a sample of 20 participants
- etc..

‚ö†Ô∏è YOU SHOULD ALWAYS HAVE MATCHING SHEETS BETWEEN THE BLANK DATASET AND THE HUMAN-CODED DATA!
For example, if you create a new tab/sheet for testing 40 participants, you must have a sheet in both the full dataset and human-coded data with corresponding participants (Pno).


---


_example_

`legaldetainment_347p_ALL.xlsx, sheet = ALL(3)`
| Pno | Qual |
|:-----------|:------------:|
| 200 | Lorem ipsum  |
| 35 | dolor sit amet  |
| 298 | Duis aute irure  |

`legaldetainment_blankdata.xlsx, sheet = ALL(3)`
| Pno | Qual |
|:-----------|:------------:|
| 200 | Lorem ipsum  |
| 35 | dolor sit amet  |
| 298 | Duis aute irure  |

`Same Pnos in both datasets ‚úÖ`


‚ùî Still unsure? the last block of this section can check this for you

### Materials
"""

with open("/content/legaldetainment/legaldetainment_story.md", "r", encoding="utf-8") as f:
    study_context = f.read()


with open("/content/legaldetainment/legaldetainment_codinginstructions.md", "r", encoding="utf-8") as f:
    coding_manual = f.read()

"""### Blank dataset"""

data = pd.read_excel("/content/legaldetainment/legaldetainment_blankdata.xlsx", sheet_name = 'ALL(10)') # üìã for cell H & G

data = data.dropna(subset=["Pno"])
data.columns = data.columns.str.strip()

print(f"Dataset size: {data.shape}")

"""### Human-coded (comparison) dataset"""

humandata = pd.read_excel("/content/legaldetainment/legaldetainment_347p_ALL.xlsx", sheet_name = 'ALL(10)') # üìã for cell I

humandata = humandata.dropna(subset=["Pno"])
humandata = humandata.replace({0: "No", 1: "Yes", "0": "No", "1": "Yes"})
humandata = humandata.rename(columns={
    "NoInformation/Evidence": "NoInformation_Evidence",
    "Unclassified/Other": "Unclassified_Other"
})
humandata.columns = humandata.columns.str.strip()

print("‚úîÔ∏è Data loading done")

"""# 3Ô∏è‚É£ Model loading"""

device = 0 if torch.cuda.is_available() else -1

CONFIDENCE_THRESHOLD = 0.4 # ‚úèÔ∏è Higher = more conservative (more blanks). Lower = more guesses


# ‚ÑπÔ∏è ‚úèÔ∏è model dependent (see line 20-21, must match)
classifier = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli",
                      device=device)
print("‚úîÔ∏è Model loading done")

"""# 4Ô∏è‚É£ Model running

v2
"""

results = []
start_time = time.time()

# ‚ûã Loop through participant responses
for i, row in data.iterrows():
    text = row["Qual"]
    result_row = row.to_dict()

    # Handle empty or missing text
    if not isinstance(text, str) or not text.strip():
        for col in binary_columns:
            result_row[col] = "No"
            result_row[f"{col}_score"] = 0.0 # NEW
        results.append(result_row)
        continue

    # ‚ûç Classify binary columns
    scores_record = {}
    for col in binary_columns:
        response = classifier(text, candidate_labels=["Yes", "No"])
        best_label = response["labels"][0]
        best_score = response["scores"][0]
        result_row[col] = best_label if best_score >= CONFIDENCE_THRESHOLD else "No"

        result_row[f"{col}_score_Yes"] = response["scores"][response["labels"].index("Yes")]
        result_row[f"{col}_score_No"] = response["scores"][response["labels"].index("No")]
        scores_record[col] = result_row[f"{col}_score_Yes"]

        # scores_record[col] = best_score

    # ‚ûé Enforce exclusivity within cluster codes
    yes_cluster = [c for c in cluster_codes if result_row[c] == "Yes"]

    if len(yes_cluster) > 1:
        # Keep only the one with highest confidence
        best = max(yes_cluster, key=lambda c: scores_record[c])
        for c in cluster_codes:
            result_row[c] = "Yes" if c == best else "No"

    # ‚ûè Enforce Unclassified_Other rule
    if all(result_row[c] == "No" for c in cluster_codes + independent_codes):
        result_row["Unclassified_Other"] = "Yes"
    else:
        result_row["Unclassified_Other"] = "No"

    results.append(result_row)

    if (i + 1) % 10 == 0:
        print(f"Processed {i + 1}/{len(data)} responses...") # ‚ÑπÔ∏è keeps us posted on progress

# ‚ûé Timer end
end_time = time.time()
elapsed = end_time - start_time
avg_time = elapsed / len(data)
print(f"\n‚úÖ Classification completed in {elapsed:.2f} seconds.")
print(f"Average time per entry: {avg_time:.3f} seconds.")

print("‚úîÔ∏è Running completed")

"""# 5Ô∏è‚É£Saving outputs"""

output_path = "/content/legaldetainment/RESULTS/10p_CF04_scores.csv" # ‚úèÔ∏è must rename for each run (+1 for example)
output_df = pd.DataFrame(results)
os.makedirs(os.path.dirname(output_path), exist_ok=True)
output_df.to_csv(output_path, index=False)
print(f"‚úîÔ∏è Results saved to {output_path}")

"""# 6Ô∏è‚É£ Evaluation"""

humandata = humandata.replace({'0': 'No', '1': 'Yes'})

# Clean up column names in output_df by stripping whitespace
output_df.columns = output_df.columns.str.strip()

# Add print statements to check column names before merge
print("Columns in humandata before merge:")
print(humandata.columns)
print("\nColumns in output_df before merge:")
print(output_df.columns)


# Compare human and model coding
eval_df = humandata[["Pno"] + all_codes].merge(
    output_df[["Pno"] + all_codes],
    on="Pno",
    how="inner"
)

# Boolean: all codes match for that Pno
eval_df["all_match"] = eval_df.apply(
    lambda row: all(row[code + "_x"] == row[code + "_y"] for code in all_codes),
    axis=1
)

# Count matches
total = len(eval_df)
matches = eval_df["all_match"].sum()
percentage = matches / total * 100 if total else 0

print(f"\nTotal Pnos compared: {total}")
print(f"Matching on all codes: {matches} ({percentage:.1f}%)") # üìã for cell N & O

# Print Pnos that match on all codes
matching_pnos = eval_df[eval_df['all_match'] == True]['Pno'].tolist()
print(f"Pnos matching on all codes: {matching_pnos}") # üìã for cell P